{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3a6155c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcp/miniconda3/envs/ai/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████| 665/665 [00:00<00:00, 266kB/s]\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████| 548M/548M [00:30<00:00, 18.0MB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████| 1.04M/1.04M [00:01<00:00, 548kB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████| 456k/456k [00:01<00:00, 324kB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████| 1.36M/1.36M [00:01<00:00, 790kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(task=\"text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41a3a0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/home/bcp/miniconda3/envs/ai/lib/python3.10/site-packages/transformers/generation/utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 50 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone. Seven Rings for the Dwarves and Men under the sun, Seven for the Elves under the dark and cold moon. Seven Swords for'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\n",
    "    \"Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone\"\n",
    ")  # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "707607ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': 'Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone, Seven for the Dragon-lords in their dungeons)\\n\\nEach player plays a different Elf-born of which the most important is'}],\n",
       " [{'generated_text': 'Nine for Mortal Men, doomed to die, One for the Dark Lord on his dark throne. There he met the Demon King Niv Vaiya, of whom he was slain just before he left the city.\\n\\nAs part of a deal'}]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\n",
    "    [\n",
    "        \"Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone\",\n",
    "        \"Nine for Mortal Men, doomed to die, One for the Dark Lord on his dark throne\",\n",
    "    ]\n",
    ")  # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22c56119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone, Eight for their city walls, Nine for their royal chambers. The most important one is to bring forth knowledge in the name of the'},\n",
       " {'generated_text': \"Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone, Eight for the Dwarves in their halls beyond the plain. Seven Rings: I'll go, to wherever I can.\\n\\n\"}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\n",
    "    \"Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone\",\n",
    "    num_return_sequences=2,\n",
    ")  # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c02f521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████| 762/762 [00:00<00:00, 302kB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████| 1.04M/1.04M [00:01<00:00, 735kB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████| 456k/456k [00:01<00:00, 338kB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████| 1.36M/1.36M [00:02<00:00, 670kB/s]\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████| 353M/353M [00:05<00:00, 68.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bb5d9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(task=\"text-generation\", \n",
    "                     model=model,\n",
    "                     tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b6692d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/home/bcp/miniconda3/envs/ai/lib/python3.10/site-packages/transformers/generation/utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 50 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone.\\nCrest and Flame - An Elven-kings who use dragonwood to power themselves - wield three blades, one in the'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\n",
    "    \"Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone\"\n",
    ")  # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aeaefae4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████████████████████████████████████████████████████████████| 5.17k/5.17k [00:00<00:00, 1.54MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset librispeech_asr_demo/clean to /home/bcp/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_demo/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files:   0%|                                                                                     | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|                                                                                     | 0.00/9.08M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   0%|                                                                            | 1.02k/9.08M [00:00<35:41, 4.24kB/s]\u001b[A\n",
      "Downloading data:   0%|▎                                                                           | 34.8k/9.08M [00:00<01:53, 79.4kB/s]\u001b[A\n",
      "Downloading data:   1%|▉                                                                             | 104k/9.08M [00:00<00:55, 162kB/s]\u001b[A\n",
      "Downloading data:   2%|█▊                                                                            | 209k/9.08M [00:01<00:34, 258kB/s]\u001b[A\n",
      "Downloading data:   5%|███▉                                                                          | 453k/9.08M [00:01<00:17, 499kB/s]\u001b[A\n",
      "Downloading data:  10%|████████                                                                      | 940k/9.08M [00:01<00:08, 954kB/s]\u001b[A\n",
      "Downloading data:  21%|███████████████▉                                                            | 1.90M/9.08M [00:01<00:03, 1.82MB/s]\u001b[A\n",
      "Downloading data:  41%|███████████████████████████████▎                                            | 3.74M/9.08M [00:01<00:01, 4.31MB/s]\u001b[A\n",
      "Downloading data:  49%|█████████████████████████████████████▎                                      | 4.45M/9.08M [00:02<00:01, 4.10MB/s]\u001b[A\n",
      "Downloading data:  75%|█████████████████████████████████████████████████████████                   | 6.82M/9.08M [00:02<00:00, 5.96MB/s]\u001b[A\n",
      "Downloading data: 100%|████████████████████████████████████████████████████████████████████████████| 9.08M/9.08M [00:02<00:00, 3.64MB/s]\u001b[A\n",
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.83s/it]\n",
      "Extracting data files: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.33it/s]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset librispeech_asr_demo downloaded and prepared to /home/bcp/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_demo/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_demo\",\n",
    "                  \"clean\",\n",
    "                  split=\"validation\")\n",
    "audio_file = ds[0][\"audio\"][\"path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "478414d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████| 2.28k/2.28k [00:00<00:00, 967kB/s]\n",
      "/home/bcp/miniconda3/envs/ai/lib/python3.10/site-packages/transformers/configuration_utils.py:369: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████| 1.27G/1.27G [01:27<00:00, 14.5MB/s]\n",
      "Some weights of the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition were not used when initializing Wav2Vec2ForSequenceClassification: ['classifier.output.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.output.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition and are newly initialized: ['projector.weight', 'classifier.bias', 'classifier.weight', 'projector.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████| 214/214 [00:00<00:00, 89.2kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "audio_classifier = pipeline(\n",
    "    task=\"audio-classification\",\n",
    "    model=\"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce63df41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.1315, 'label': 'calm'},\n",
       " {'score': 0.1307, 'label': 'neutral'},\n",
       " {'score': 0.1274, 'label': 'sad'},\n",
       " {'score': 0.1261, 'label': 'fearful'},\n",
       " {'score': 0.1242, 'label': 'happy'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = audio_classifier(audio_file)\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e56a31c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google/vit-base-patch16-224 and revision 5dca96d (https://huggingface.co/google/vit-base-patch16-224).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████| 69.7k/69.7k [00:00<00:00, 124kB/s]\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████| 346M/346M [00:05<00:00, 66.6MB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████| 160/160 [00:00<00:00, 53.8kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.4335, 'label': 'lynx, catamount'},\n",
       " {'score': 0.0348,\n",
       "  'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'},\n",
       " {'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'},\n",
       " {'score': 0.0239, 'label': 'Egyptian cat'},\n",
       " {'score': 0.0229, 'label': 'tiger cat'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "vision_classifier = pipeline(task=\"image-classification\")\n",
    "preds = vision_classifier(\n",
    "    images=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n",
    ")\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0382c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n",
    "question = \"Where is the cat?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bff5c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dandelin/vilt-b32-finetuned-vqa and revision 4355f59 (https://huggingface.co/dandelin/vilt-b32-finetuned-vqa).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████| 136k/136k [00:00<00:00, 159kB/s]\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████| 470M/470M [00:06<00:00, 67.7MB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████| 320/320 [00:00<00:00, 117kB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████| 232k/232k [00:01<00:00, 206kB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████| 466k/466k [00:01<00:00, 335kB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:00<00:00, 47.5kB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████| 251/251 [00:00<00:00, 112kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.911, 'answer': 'snow'},\n",
       " {'score': 0.8786, 'answer': 'in snow'},\n",
       " {'score': 0.6714, 'answer': 'outside'},\n",
       " {'score': 0.0293, 'answer': 'on ground'},\n",
       " {'score': 0.0272, 'answer': 'ground'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "vqa = pipeline(task=\"vqa\")\n",
    "preds = vqa(image=image, question=question)\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"answer\": pred[\"answer\"]} for pred in preds]\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df729150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
