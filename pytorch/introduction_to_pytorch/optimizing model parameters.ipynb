{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9abcce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    training_data, batch_size=64\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_data, batch_size=64\n",
    ")\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84088735",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee2a6e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b136d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af48d445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "    \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f72f5255",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 1\n",
      " ----------------------\n",
      "loss: 2.313511 [    0/60000]\n",
      "loss: 2.297167 [ 6400/60000]\n",
      "loss: 2.281606 [12800/60000]\n",
      "loss: 2.268901 [19200/60000]\n",
      "loss: 2.251368 [25600/60000]\n",
      "loss: 2.232074 [32000/60000]\n",
      "loss: 2.237199 [38400/60000]\n",
      "loss: 2.212439 [44800/60000]\n",
      "loss: 2.197407 [51200/60000]\n",
      "loss: 2.173473 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 48.0%, Avg loss: 2.167256 \n",
      "\n",
      "Epochs 2\n",
      " ----------------------\n",
      "loss: 2.181800 [    0/60000]\n",
      "loss: 2.168237 [ 6400/60000]\n",
      "loss: 2.119170 [12800/60000]\n",
      "loss: 2.125786 [19200/60000]\n",
      "loss: 2.079686 [25600/60000]\n",
      "loss: 2.030833 [32000/60000]\n",
      "loss: 2.052356 [38400/60000]\n",
      "loss: 1.986548 [44800/60000]\n",
      "loss: 1.972642 [51200/60000]\n",
      "loss: 1.911683 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 1.909178 \n",
      "\n",
      "Epochs 3\n",
      " ----------------------\n",
      "loss: 1.946215 [    0/60000]\n",
      "loss: 1.912132 [ 6400/60000]\n",
      "loss: 1.806074 [12800/60000]\n",
      "loss: 1.833210 [19200/60000]\n",
      "loss: 1.730707 [25600/60000]\n",
      "loss: 1.686114 [32000/60000]\n",
      "loss: 1.704404 [38400/60000]\n",
      "loss: 1.616711 [44800/60000]\n",
      "loss: 1.619377 [51200/60000]\n",
      "loss: 1.522550 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Avg loss: 1.541559 \n",
      "\n",
      "Epochs 4\n",
      " ----------------------\n",
      "loss: 1.612282 [    0/60000]\n",
      "loss: 1.571314 [ 6400/60000]\n",
      "loss: 1.434400 [12800/60000]\n",
      "loss: 1.492673 [19200/60000]\n",
      "loss: 1.376606 [25600/60000]\n",
      "loss: 1.372195 [32000/60000]\n",
      "loss: 1.387585 [38400/60000]\n",
      "loss: 1.320017 [44800/60000]\n",
      "loss: 1.336939 [51200/60000]\n",
      "loss: 1.240099 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 1.270783 \n",
      "\n",
      "Epochs 5\n",
      " ----------------------\n",
      "loss: 1.356740 [    0/60000]\n",
      "loss: 1.328410 [ 6400/60000]\n",
      "loss: 1.176552 [12800/60000]\n",
      "loss: 1.267612 [19200/60000]\n",
      "loss: 1.143621 [25600/60000]\n",
      "loss: 1.168153 [32000/60000]\n",
      "loss: 1.192619 [38400/60000]\n",
      "loss: 1.133651 [44800/60000]\n",
      "loss: 1.159140 [51200/60000]\n",
      "loss: 1.075912 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.102143 \n",
      "\n",
      "Epochs 6\n",
      " ----------------------\n",
      "loss: 1.185817 [    0/60000]\n",
      "loss: 1.174387 [ 6400/60000]\n",
      "loss: 1.004037 [12800/60000]\n",
      "loss: 1.126666 [19200/60000]\n",
      "loss: 0.999340 [25600/60000]\n",
      "loss: 1.031735 [32000/60000]\n",
      "loss: 1.073710 [38400/60000]\n",
      "loss: 1.013243 [44800/60000]\n",
      "loss: 1.042584 [51200/60000]\n",
      "loss: 0.974292 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 0.993259 \n",
      "\n",
      "Epochs 7\n",
      " ----------------------\n",
      "loss: 1.066058 [    0/60000]\n",
      "loss: 1.074671 [ 6400/60000]\n",
      "loss: 0.885227 [12800/60000]\n",
      "loss: 1.032851 [19200/60000]\n",
      "loss: 0.909669 [25600/60000]\n",
      "loss: 0.936623 [32000/60000]\n",
      "loss: 0.997175 [38400/60000]\n",
      "loss: 0.933558 [44800/60000]\n",
      "loss: 0.961747 [51200/60000]\n",
      "loss: 0.906701 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.919146 \n",
      "\n",
      "Epochs 8\n",
      " ----------------------\n",
      "loss: 0.977041 [    0/60000]\n",
      "loss: 1.005660 [ 6400/60000]\n",
      "loss: 0.800310 [12800/60000]\n",
      "loss: 0.966426 [19200/60000]\n",
      "loss: 0.851005 [25600/60000]\n",
      "loss: 0.867484 [32000/60000]\n",
      "loss: 0.943562 [38400/60000]\n",
      "loss: 0.879237 [44800/60000]\n",
      "loss: 0.903433 [51200/60000]\n",
      "loss: 0.858853 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.865912 \n",
      "\n",
      "Epochs 9\n",
      " ----------------------\n",
      "loss: 0.908101 [    0/60000]\n",
      "loss: 0.954098 [ 6400/60000]\n",
      "loss: 0.737319 [12800/60000]\n",
      "loss: 0.916649 [19200/60000]\n",
      "loss: 0.809799 [25600/60000]\n",
      "loss: 0.816161 [32000/60000]\n",
      "loss: 0.903015 [38400/60000]\n",
      "loss: 0.841078 [44800/60000]\n",
      "loss: 0.860007 [51200/60000]\n",
      "loss: 0.822679 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.825748 \n",
      "\n",
      "Epochs 10\n",
      " ----------------------\n",
      "loss: 0.853090 [    0/60000]\n",
      "loss: 0.912970 [ 6400/60000]\n",
      "loss: 0.688915 [12800/60000]\n",
      "loss: 0.878071 [19200/60000]\n",
      "loss: 0.778903 [25600/60000]\n",
      "loss: 0.776773 [32000/60000]\n",
      "loss: 0.870111 [38400/60000]\n",
      "loss: 0.812984 [44800/60000]\n",
      "loss: 0.826259 [51200/60000]\n",
      "loss: 0.793674 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.793875 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epochs {t+1}\\n ----------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
